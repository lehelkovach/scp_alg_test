#!/usr/bin/env python3
"""
Hallucination Detection Benchmark Suite
========================================

Author: Lehel Kovach
AI Assistant: Claude Opus 4.5 (Anthropic)
Repository: https://github.com/lehelkovach/scp_alg_test

Benchmarks all hallucination detection algorithms with multiple test sets.

Each algorithm is tested for:
- Correctness (pass/fail on known claims)
- Latency (time per verification)
- Availability (can it run without external dependencies?)

Usage:
    python benchmark.py                    # Run all benchmarks
    python benchmark.py --algorithm scp    # Run specific algorithm
    python benchmark.py --verbose          # Show detailed output
    python benchmark.py --export           # Export results to markdown
"""

import sys
import os
import time
import json
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional, Callable, Tuple
from enum import Enum
from datetime import datetime

sys.path.insert(0, os.path.dirname(__file__))

__author__ = "Lehel Kovach"
__ai_assistant__ = "Claude Opus 4.5"


# =============================================================================
# TEST PARAMETER SETS - Organized by Hallucination Type
# =============================================================================

# Each test is: (claim_text, is_true, hallucination_type)
# hallucination_type: "true_fact", "false_attribution", "contradiction", "fabrication"

class HallucinationType(Enum):
    TRUE_FACT = "true_fact"              # Correct, verifiable fact
    FALSE_ATTRIBUTION = "false_attr"     # Wrong subject for correct predicate/object
    CONTRADICTION = "contradiction"       # Directly conflicts with known fact
    FABRICATION = "fabrication"          # Completely made up, not in any KB
    EXTRINSIC = "extrinsic"              # Added info not supported by source


# =============================================================================
# IMPLEMENTATION GUIDANCE - What to build when tests fail
# =============================================================================

IMPLEMENTATION_GUIDANCE = {
    # Per hallucination type - what's needed to detect it
    HallucinationType.FALSE_ATTRIBUTION: {
        "description": "Detect wrong subject for correct predicate/object",
        "example": '"Edison invented telephone" should fail (Bell did)',
        "fix": """
IMPLEMENTATION NEEDED: Subject-Aware Matching
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
In scp.py, update find_similar_facts() to compare subjects:

    def find_similar_facts(self, claim, threshold=0.7):
        matches = super().find_similar_facts(claim, threshold)
        for score, (subj, pred, obj), rel_id in matches:
            # If predicate+object match but subject differs = FALSE ATTRIBUTION
            if (self._normalize(claim.predicate) == self._normalize(pred) and
                self._normalize(claim.obj) == self._normalize(obj) and
                self._normalize(claim.subject) != self._normalize(subj)):
                return [(0.0, (subj, pred, obj), rel_id)]  # Return as contradiction
        return matches

Estimated time: 1-2 hours
""",
    },
    HallucinationType.CONTRADICTION: {
        "description": "Detect claims that conflict with known facts",
        "example": '"Einstein born in France" should fail (born in Germany)',
        "fix": """
IMPLEMENTATION NEEDED: Expand Predicate Support
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
For Wikidata (wikidata_verifier.py), add more predicates:

    PREDICATE_MAPPING = {
        "invented": "P61",
        "discovered": "P61", 
        "born_in": "P19",        # ADD: place of birth
        "died_in": "P20",        # ADD: place of death
        "located_in": "P131",    # ADD: located in territory
        "capital_of": "P36",     # ADD: capital
    }

For SCP, ensure KB has the facts:
    kb.add_fact("Einstein", "born_in", "Germany")

Estimated time: 1-2 hours
""",
    },
    HallucinationType.FABRICATION: {
        "description": "Detect completely made-up facts",
        "example": '"Curie invented smartphone" should fail (never happened)',
        "fix": """
IMPLEMENTATION NEEDED: Unknown Fact Detection
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Should return FAIL/REFUTED for claims with no KB match.

For SCP: Already implemented (returns FAIL for no match)
For Wikidata: Add fallback when SPARQL returns empty:

    if not results:
        return WikidataResult(
            status=VerificationStatus.REFUTED,
            reason="No evidence found in Wikidata"
        )

Estimated time: 30 minutes
""",
    },
    HallucinationType.TRUE_FACT: {
        "description": "Correctly verify true claims",
        "example": '"Bell invented telephone" should pass',
        "fix": """
IMPLEMENTATION NEEDED: Expand Knowledge Base
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Add more facts to the KB:

    kb.add_facts_bulk([
        ("Einstein", "born_in", "Germany"),
        ("Python", "created_by", "Guido van Rossum"),
        ("Great Wall", "located_in", "China"),
    ])

Or use external KB (Wikidata) for broader coverage.

Estimated time: 30 minutes (manual) or 4-8 hours (auto-import)
""",
    },
}

# Per algorithm - specific implementation needs
ALGORITHM_IMPLEMENTATION_NEEDS = {
    "SCP": {
        "false_attr_fix": """
FIX: Update SCPProber._probe_claim() in scp.py:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Add subject comparison after finding semantic match:

    if best_match and best_score > self.soft_threshold:
        match_subj, match_pred, match_obj = best_match
        # Check if this is actually a false attribution
        if (claim.predicate == match_pred and claim.obj == match_obj 
            and claim.subject != match_subj):
            return ProbeResult(
                claim=claim,
                verdict=Verdict.CONTRADICT,
                score=0.0,
                reason=f"False attribution: {match_subj} {match_pred} {match_obj}, not {claim.subject}"
            )
""",
    },
    "Wikidata": {
        "predicate_fix": """
FIX: Add predicates to wikidata_verifier.py:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PREDICATE_MAPPING = {
    # Existing
    "invented": "P61",
    "discovered": "P61",
    # Add these:
    "born_in": "P19",
    "located_in": "P131", 
    "capital_of": "P36",
    "created_by": "P170",
    "founded_by": "P112",
}

Then add SPARQL templates for each.
""",
    },
    "LLM-Judge": {
        "api_fix": """
FIX: Add real LLM in hallucination_strategies.py:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Set environment variable:
   export OPENAI_API_KEY=sk-...

2. Replace mock_llm with:
   
   import openai
   
   def real_llm(prompt: str) -> str:
       response = openai.ChatCompletion.create(
           model="gpt-4o-mini",
           messages=[{"role": "user", "content": prompt}]
       )
       return response.choices[0].message.content

3. Update LLMJudgeStrategy to use real_llm
""",
    },
    "KnowShowGo": {
        "server_fix": """
FIX: Deploy KnowShowGo server:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Clone: git clone https://github.com/lehelkovach/knowshowgo
2. Install: npm install
3. Run: npm start
4. Set: export KSG_URL=http://localhost:3000

See: docs/knowshowgo_integration_spec.md
""",
    },
}


# =============================================================================
# TEST SET 1: FALSE ATTRIBUTION (10+ tests)
# Tests if algorithm can detect wrong subject attribution
# e.g., "Edison invented telephone" when Bell did
# =============================================================================
TEST_FALSE_ATTRIBUTION = [
    # (claim, is_true, type)
    # --- Inventions ---
    ("Alexander Graham Bell invented the telephone.", True, HallucinationType.TRUE_FACT),
    ("Thomas Edison invented the telephone.", False, HallucinationType.FALSE_ATTRIBUTION),
    ("Thomas Edison invented the light bulb.", True, HallucinationType.TRUE_FACT),
    ("Nikola Tesla invented the light bulb.", False, HallucinationType.FALSE_ATTRIBUTION),
    ("Guglielmo Marconi invented the radio.", True, HallucinationType.TRUE_FACT),
    ("Thomas Edison invented the radio.", False, HallucinationType.FALSE_ATTRIBUTION),
    
    # --- Discoveries ---
    ("Albert Einstein discovered the theory of relativity.", True, HallucinationType.TRUE_FACT),
    ("Isaac Newton discovered the theory of relativity.", False, HallucinationType.FALSE_ATTRIBUTION),
    ("Marie Curie discovered radium.", True, HallucinationType.TRUE_FACT),
    ("Nikola Tesla discovered radium.", False, HallucinationType.FALSE_ATTRIBUTION),
    ("Isaac Newton discovered gravity.", True, HallucinationType.TRUE_FACT),
    ("Galileo discovered gravity.", False, HallucinationType.FALSE_ATTRIBUTION),
    
    # --- Founding/Creation ---
    ("Bill Gates founded Microsoft.", True, HallucinationType.TRUE_FACT),
    ("Steve Jobs founded Microsoft.", False, HallucinationType.FALSE_ATTRIBUTION),
    ("Mark Zuckerberg founded Facebook.", True, HallucinationType.TRUE_FACT),
    ("Elon Musk founded Facebook.", False, HallucinationType.FALSE_ATTRIBUTION),
]

# =============================================================================
# TEST SET 2: CONTRADICTIONS (10+ tests)
# Tests if algorithm can detect claims that contradict known facts
# e.g., "Einstein was born in France" when he was born in Germany
# =============================================================================
TEST_CONTRADICTIONS = [
    # --- Birth locations ---
    ("Albert Einstein was born in Germany.", True, HallucinationType.TRUE_FACT),
    ("Albert Einstein was born in France.", False, HallucinationType.CONTRADICTION),
    ("Isaac Newton was born in England.", True, HallucinationType.TRUE_FACT),
    ("Isaac Newton was born in Italy.", False, HallucinationType.CONTRADICTION),
    
    # --- Landmark locations ---
    ("The Eiffel Tower is located in Paris.", True, HallucinationType.TRUE_FACT),
    ("The Eiffel Tower is located in London.", False, HallucinationType.CONTRADICTION),
    ("The Statue of Liberty is located in New York.", True, HallucinationType.TRUE_FACT),
    ("The Statue of Liberty is located in Boston.", False, HallucinationType.CONTRADICTION),
    ("The Colosseum is located in Rome.", True, HallucinationType.TRUE_FACT),
    ("The Colosseum is located in Athens.", False, HallucinationType.CONTRADICTION),
    
    # --- Capital cities ---
    ("Tokyo is the capital of Japan.", True, HallucinationType.TRUE_FACT),
    ("Tokyo is the capital of China.", False, HallucinationType.CONTRADICTION),
    ("Paris is the capital of France.", True, HallucinationType.TRUE_FACT),
    ("Paris is the capital of Spain.", False, HallucinationType.CONTRADICTION),
    ("Berlin is the capital of Germany.", True, HallucinationType.TRUE_FACT),
    ("Berlin is the capital of Austria.", False, HallucinationType.CONTRADICTION),
]

# =============================================================================
# TEST SET 3: FABRICATIONS (10+ tests)
# Tests if algorithm can detect completely made up facts
# e.g., "Einstein invented the internet"
# =============================================================================
TEST_FABRICATIONS = [
    # --- Tech fabrications ---
    ("Python was created by Guido van Rossum.", True, HallucinationType.TRUE_FACT),
    ("Einstein invented the internet.", False, HallucinationType.FABRICATION),
    ("Tim Berners-Lee invented the World Wide Web.", True, HallucinationType.TRUE_FACT),
    ("Albert Einstein invented the computer.", False, HallucinationType.FABRICATION),
    
    # --- Historical fabrications ---
    ("Marie Curie discovered radium.", True, HallucinationType.TRUE_FACT),
    ("Marie Curie invented the smartphone.", False, HallucinationType.FABRICATION),
    ("The Great Wall is located in China.", True, HallucinationType.TRUE_FACT),
    ("The Great Wall was built by Napoleon.", False, HallucinationType.FABRICATION),
    ("Shakespeare wrote Hamlet.", True, HallucinationType.TRUE_FACT),
    ("Shakespeare invented the printing press.", False, HallucinationType.FABRICATION),
    
    # --- Absurd fabrications ---
    ("Water is composed of hydrogen and oxygen.", True, HallucinationType.TRUE_FACT),
    ("Isaac Newton invented time travel.", False, HallucinationType.FABRICATION),
    ("The moon orbits the Earth.", True, HallucinationType.TRUE_FACT),
    ("Julius Caesar discovered Antarctica.", False, HallucinationType.FABRICATION),
    ("Leonardo da Vinci painted the Mona Lisa.", True, HallucinationType.TRUE_FACT),
    ("Leonardo da Vinci invented the airplane.", False, HallucinationType.FABRICATION),
]

# =============================================================================
# TEST SET 4: EDGE CASES
# Tests boundary conditions and tricky cases
# =============================================================================
TEST_EDGE_CASES = [
    # --- Partial matches (should still work) ---
    ("Bell invented telephone.", True, HallucinationType.TRUE_FACT),
    ("Einstein relativity.", True, HallucinationType.TRUE_FACT),  # Incomplete but true
    
    # --- Synonyms ---
    ("Edison created the lightbulb.", True, HallucinationType.TRUE_FACT),  # created vs invented
    ("Newton found gravity.", True, HallucinationType.TRUE_FACT),  # found vs discovered
    
    # --- Near-misses (similar but wrong) ---
    ("Thomas Edison discovered electricity.", False, HallucinationType.FABRICATION),  # close but wrong
    ("Marie Curie invented polonium.", False, HallucinationType.FALSE_ATTRIBUTION),  # she discovered, not invented
    
    # --- Negations ---
    ("Einstein did not discover gravity.", True, HallucinationType.TRUE_FACT),
    ("Bell did not invent the computer.", True, HallucinationType.TRUE_FACT),
]

# =============================================================================
# TEST SET 5: DOMAIN COVERAGE
# Tests across different knowledge domains
# =============================================================================
TEST_DOMAINS = [
    # --- Science ---
    ("Darwin proposed the theory of evolution.", True, HallucinationType.TRUE_FACT),
    ("Lamarck proposed the theory of evolution.", False, HallucinationType.FALSE_ATTRIBUTION),
    
    # --- Technology ---
    ("Steve Jobs co-founded Apple.", True, HallucinationType.TRUE_FACT),
    ("Bill Gates co-founded Apple.", False, HallucinationType.FALSE_ATTRIBUTION),
    
    # --- History ---
    ("George Washington was the first US president.", True, HallucinationType.TRUE_FACT),
    ("Abraham Lincoln was the first US president.", False, HallucinationType.FALSE_ATTRIBUTION),
    
    # --- Geography ---
    ("Mount Everest is the tallest mountain.", True, HallucinationType.TRUE_FACT),
    ("K2 is the tallest mountain.", False, HallucinationType.CONTRADICTION),
    
    # --- Arts ---
    ("Beethoven composed the 9th Symphony.", True, HallucinationType.TRUE_FACT),
    ("Mozart composed the 9th Symphony.", False, HallucinationType.FALSE_ATTRIBUTION),
]

# All test sets with their descriptions
TEST_SETS = {
    "false_attribution": ("False Attribution Detection", TEST_FALSE_ATTRIBUTION),
    "contradictions": ("Contradiction Detection", TEST_CONTRADICTIONS),
    "fabrications": ("Fabrication Detection", TEST_FABRICATIONS),
    "edge_cases": ("Edge Cases", TEST_EDGE_CASES),
    "domains": ("Domain Coverage", TEST_DOMAINS),
}

# Legacy format for backward compatibility (claim, is_true) tuples
def get_test_tuples(test_set):
    """Convert to simple (claim, is_true) format."""
    return [(claim, is_true) for claim, is_true, _ in test_set]


# =============================================================================
# RESULT DATA STRUCTURES
# =============================================================================

class AlgorithmStatus(Enum):
    AVAILABLE = "available"
    UNAVAILABLE = "unavailable"
    MOCK_ONLY = "mock_only"


class CoverageLevel(Enum):
    """How comprehensive the hallucination detection is."""
    EXCELLENT = "excellent"   # 90%+ accuracy, detects all hallucination types
    GOOD = "good"             # 70-89% accuracy, detects most types
    MODERATE = "moderate"     # 50-69% accuracy, limited detection
    WEAK = "weak"             # 30-49% accuracy, unreliable
    MINIMAL = "minimal"       # <30% or unavailable


@dataclass
class CoverageMetrics:
    """Coverage metrics for an algorithm."""
    level: CoverageLevel
    score: float                    # 0-100 coverage score
    detects_false_attribution: bool # Can detect "Edison invented telephone"
    detects_contradictions: bool    # Can detect contradicting claims
    detects_extrinsic: bool         # Can detect info not in KB
    detects_intrinsic: bool         # Can detect modified facts
    domain_coverage: Dict[str, float]  # Coverage per domain
    strengths: List[str]
    weaknesses: List[str]


@dataclass
class ClaimResult:
    claim: str
    expected: bool
    actual: Optional[bool]
    passed: bool
    latency_ms: float
    verdict: str
    reason: str
    hallucination_type: HallucinationType


@dataclass
class TypeDetectionRate:
    """Detection rate for a specific hallucination type."""
    hallucination_type: HallucinationType
    tested: int
    detected: int
    rate: float


@dataclass
class TestSetResult:
    name: str
    claims_tested: int
    passed: int
    failed: int
    accuracy: float
    avg_latency_ms: float
    total_time_ms: float
    results: List[ClaimResult]
    detection_by_type: Dict[HallucinationType, TypeDetectionRate]


@dataclass
class AlgorithmResult:
    name: str
    description: str
    status: AlgorithmStatus
    status_reason: str
    test_sets: Dict[str, TestSetResult]
    overall_accuracy: float
    overall_latency_ms: float
    coverage: Optional[CoverageMetrics]
    timestamp: str


# =============================================================================
# ALGORITHM IMPLEMENTATIONS
# =============================================================================

class Algorithm:
    """Base class for hallucination detection algorithms."""
    
    name: str = "base"
    description: str = "Base algorithm"
    
    # Coverage capabilities (override in subclasses)
    detects_false_attribution: bool = False
    detects_contradictions: bool = False
    detects_extrinsic: bool = False
    detects_intrinsic: bool = False
    strengths: List[str] = []
    weaknesses: List[str] = []
    
    def __init__(self):
        self.status = AlgorithmStatus.AVAILABLE
        self.status_reason = "Ready"
        self._setup()
    
    def _setup(self):
        """Initialize the algorithm. Override to check dependencies."""
        pass
    
    def verify(self, claim: str) -> Tuple[Optional[bool], str, str]:
        """
        Verify a claim.
        
        Returns: (is_true, verdict_name, reason)
            - is_true: True if verified, False if refuted, None if unverifiable
            - verdict_name: Name of the verdict (e.g., "PASS", "FAIL")
            - reason: Explanation of the result
        """
        raise NotImplementedError
    
    def get_coverage_metrics(
        self, 
        domain_scores: Dict[str, float], 
        overall_accuracy: float,
        actual_detection_rates: Dict[HallucinationType, float] = None
    ) -> CoverageMetrics:
        """Calculate coverage metrics based on actual test results."""
        actual_detection_rates = actual_detection_rates or {}
        
        # Use ACTUAL detection rates from tests, not declared capabilities
        actual_false_attr = actual_detection_rates.get(HallucinationType.FALSE_ATTRIBUTION, 0) >= 0.5
        actual_contradiction = actual_detection_rates.get(HallucinationType.CONTRADICTION, 0) >= 0.5
        actual_fabrication = actual_detection_rates.get(HallucinationType.FABRICATION, 0) >= 0.5
        actual_true_fact = actual_detection_rates.get(HallucinationType.TRUE_FACT, 0) >= 0.5
        
        # Determine coverage level based on overall accuracy
        if self.status == AlgorithmStatus.UNAVAILABLE:
            level = CoverageLevel.MINIMAL
        elif overall_accuracy >= 0.9:
            level = CoverageLevel.EXCELLENT
        elif overall_accuracy >= 0.7:
            level = CoverageLevel.GOOD
        elif overall_accuracy >= 0.5:
            level = CoverageLevel.MODERATE
        elif overall_accuracy >= 0.3:
            level = CoverageLevel.WEAK
        else:
            level = CoverageLevel.MINIMAL
        
        # Calculate coverage score (0-100) based on ACTUAL test results
        # Weights: accuracy (40%), actual detection rates (40%), domain coverage (20%)
        
        # Actual detection capability score from tests
        actual_capability_score = 0
        type_weights = {
            HallucinationType.FALSE_ATTRIBUTION: 0.3,  # Important
            HallucinationType.CONTRADICTION: 0.3,      # Important
            HallucinationType.FABRICATION: 0.25,       # Important
            HallucinationType.TRUE_FACT: 0.15,         # Should pass true facts
        }
        for h_type, weight in type_weights.items():
            if h_type in actual_detection_rates:
                actual_capability_score += actual_detection_rates[h_type] * weight
        
        domain_avg = sum(domain_scores.values()) / len(domain_scores) if domain_scores else 0
        
        score = (overall_accuracy * 40) + (actual_capability_score * 100 * 0.4) + (domain_avg * 100 * 0.2)
        
        return CoverageMetrics(
            level=level,
            score=score,
            detects_false_attribution=actual_false_attr,
            detects_contradictions=actual_contradiction,
            detects_extrinsic=actual_fabrication,  # fabrication tests extrinsic-like
            detects_intrinsic=actual_true_fact,    # true_fact tests intrinsic verification
            domain_coverage=domain_scores,
            strengths=self.strengths,
            weaknesses=self.weaknesses
        )


class SCPAlgorithm(Algorithm):
    """SCP (Symbolic Consistency Probing) - Local Knowledge Base."""
    
    name = "SCP"
    description = "Local KB verification using semantic embeddings (~10ms, 0 API calls)"
    
    # Coverage capabilities
    detects_false_attribution = True   # Can detect wrong subject for same predicate
    detects_contradictions = True      # Has contradiction detection
    detects_extrinsic = True           # Fails on unknown facts
    detects_intrinsic = True           # Catches modified facts
    strengths = [
        "Very fast (~10ms)",
        "Zero API calls",
        "Deterministic results",
        "Proof subgraph for auditing",
        "Contradiction detection",
    ]
    weaknesses = [
        "Limited to facts in KB",
        "Requires KB maintenance",
        "Semantic similarity can soft-match wrong subjects",
    ]
    
    def _setup(self):
        try:
            from scp import (
                HyperKB, SCPProber, RuleBasedExtractor,
                HashingEmbeddingBackend, Verdict
            )
            self.Verdict = Verdict
            
            # Create KB with comprehensive ground truth
            backend = HashingEmbeddingBackend(dim=512)
            self.kb = HyperKB(embedding_backend=backend)
            self.kb.add_facts_bulk([
                # === Inventions ===
                ("Alexander Graham Bell", "invented", "the telephone"),
                ("Thomas Edison", "invented", "the light bulb"),
                ("Thomas Edison", "invented", "the lightbulb"),
                ("Nikola Tesla", "invented", "alternating current"),
                ("Guglielmo Marconi", "invented", "the radio"),
                ("Tim Berners-Lee", "invented", "the World Wide Web"),
                ("Leonardo da Vinci", "painted", "the Mona Lisa"),
                
                # === Discoveries ===
                ("Albert Einstein", "discovered", "the theory of relativity"),
                ("Marie Curie", "discovered", "radium"),
                ("Marie Curie", "discovered", "polonium"),
                ("Isaac Newton", "discovered", "gravity"),
                ("Isaac Newton", "found", "gravity"),
                ("Charles Darwin", "proposed", "the theory of evolution"),
                
                # === Founders/Creators ===
                ("Bill Gates", "founded", "Microsoft"),
                ("Steve Jobs", "co-founded", "Apple"),
                ("Steve Jobs", "founded", "Apple"),
                ("Mark Zuckerberg", "founded", "Facebook"),
                ("Guido van Rossum", "created", "Python"),
                ("Linus Torvalds", "created", "Linux"),
                
                # === Geography - Landmarks ===
                ("The Eiffel Tower", "located_in", "Paris"),
                ("The Great Wall", "located_in", "China"),
                ("The Statue of Liberty", "located_in", "New York"),
                ("The Colosseum", "located_in", "Rome"),
                ("Mount Everest", "is", "the tallest mountain"),
                
                # === Geography - Capitals ===
                ("Tokyo", "capital_of", "Japan"),
                ("Paris", "capital_of", "France"),
                ("Berlin", "capital_of", "Germany"),
                
                # === Birth places ===
                ("Albert Einstein", "born_in", "Germany"),
                ("Isaac Newton", "born_in", "England"),
                
                # === History ===
                ("George Washington", "was", "the first US president"),
                ("Shakespeare", "wrote", "Hamlet"),
                
                # === Science ===
                ("Water", "composed_of", "hydrogen and oxygen"),
                ("The moon", "orbits", "the Earth"),
                
                # === Music ===
                ("Beethoven", "composed", "the 9th Symphony"),
            ])
            self.prober = SCPProber(
                kb=self.kb,
                extractor=RuleBasedExtractor(),
                soft_threshold=0.7
            )
            self.status = AlgorithmStatus.AVAILABLE
            self.status_reason = "Ready with local KB"
        except ImportError as e:
            self.status = AlgorithmStatus.UNAVAILABLE
            self.status_reason = f"Missing dependency: {e}"
    
    def verify(self, claim: str) -> Tuple[Optional[bool], str, str]:
        if self.status != AlgorithmStatus.AVAILABLE:
            return None, "UNAVAILABLE", self.status_reason
        
        report = self.prober.probe(claim)
        result = report.results[0]
        
        if result.verdict == self.Verdict.PASS:
            return True, "PASS", result.reason
        elif result.verdict == self.Verdict.SOFT_PASS:
            return True, "SOFT_PASS", result.reason
        elif result.verdict == self.Verdict.CONTRADICT:
            return False, "CONTRADICT", result.reason
        elif result.verdict == self.Verdict.FAIL:
            return False, "FAIL", result.reason
        else:
            return None, "UNKNOWN", result.reason


class WikidataAlgorithm(Algorithm):
    """Wikidata - External Knowledge Graph (100M+ facts)."""
    
    name = "Wikidata"
    description = "Wikidata SPARQL queries (~200ms, free public API)"
    
    # Coverage capabilities
    detects_false_attribution = True   # Can query who actually invented X
    detects_contradictions = True      # Returns correct answer vs claimed
    detects_extrinsic = False          # Only verifies, doesn't catch additions
    detects_intrinsic = True           # Catches wrong facts
    strengths = [
        "100M+ facts available",
        "No training required",
        "Structured provenance",
        "Free public API",
        "High accuracy for supported predicates",
    ]
    weaknesses = [
        "~200ms latency (network)",
        "Limited predicate support",
        "Cannot verify all claim types",
        "Rate limited",
    ]
    
    def _setup(self):
        try:
            from wikidata_verifier import WikidataVerifier, VerificationStatus
            self.verifier = WikidataVerifier()
            self.VerificationStatus = VerificationStatus
            self.status = AlgorithmStatus.AVAILABLE
            self.status_reason = "Ready with Wikidata API"
        except ImportError as e:
            self.status = AlgorithmStatus.UNAVAILABLE
            self.status_reason = f"Missing dependency: {e}"
    
    def verify(self, claim: str) -> Tuple[Optional[bool], str, str]:
        if self.status != AlgorithmStatus.AVAILABLE:
            return None, "UNAVAILABLE", self.status_reason
        
        result = self.verifier.verify(claim)
        
        if result.status == self.VerificationStatus.VERIFIED:
            return True, "VERIFIED", result.reason
        elif result.status == self.VerificationStatus.REFUTED:
            return False, "REFUTED", result.reason
        else:
            return None, "UNVERIFIABLE", result.reason


class LLMJudgeAlgorithm(Algorithm):
    """LLM-as-Judge - Uses LLM to verify claims."""
    
    name = "LLM-Judge"
    description = "LLM verification (~200ms, 1 API call per claim)"
    
    # Coverage capabilities
    detects_false_attribution = True   # LLM knows common facts
    detects_contradictions = True      # LLM can reason about contradictions
    detects_extrinsic = True           # LLM can identify fabricated info
    detects_intrinsic = True           # LLM can catch modified facts
    strengths = [
        "Broad knowledge coverage",
        "Can verify complex claims",
        "Natural language understanding",
        "No KB maintenance needed",
    ]
    weaknesses = [
        "LLM can also hallucinate",
        "Requires API key and costs $",
        "~200ms latency per call",
        "Non-deterministic",
    ]
    
    def _setup(self):
        try:
            from hallucination_strategies import LLMJudgeStrategy, mock_llm
            from scp import Verdict
            
            # Check for real LLM API
            import os
            if os.environ.get("OPENAI_API_KEY") or os.environ.get("ANTHROPIC_API_KEY"):
                self.status = AlgorithmStatus.AVAILABLE
                self.status_reason = "Ready with LLM API"
                # Would use real LLM here
                self.judge = LLMJudgeStrategy(mock_llm)
            else:
                self.status = AlgorithmStatus.MOCK_ONLY
                self.status_reason = "Using mock LLM (set OPENAI_API_KEY for real)"
                self.judge = LLMJudgeStrategy(mock_llm)
            
            self.Verdict = Verdict
        except ImportError as e:
            self.status = AlgorithmStatus.UNAVAILABLE
            self.status_reason = f"Missing dependency: {e}"
    
    def verify(self, claim: str) -> Tuple[Optional[bool], str, str]:
        if self.status == AlgorithmStatus.UNAVAILABLE:
            return None, "UNAVAILABLE", self.status_reason
        
        result = self.judge.check(claim)
        
        if result.verdict == self.Verdict.PASS:
            return True, "PASS", result.reasoning
        elif result.verdict == self.Verdict.FAIL:
            return False, "FAIL", result.reasoning
        else:
            return None, "UNKNOWN", result.reasoning


class SelfConsistencyAlgorithm(Algorithm):
    """Self-Consistency - Multiple LLM samples for consensus."""
    
    name = "Self-Consistency"
    description = "Multiple LLM samples (~500ms, 3-5 API calls)"
    
    # Coverage capabilities
    detects_false_attribution = True
    detects_contradictions = True
    detects_extrinsic = True
    detects_intrinsic = True
    strengths = [
        "More robust than single LLM call",
        "Catches LLM inconsistencies",
        "Better accuracy through voting",
    ]
    weaknesses = [
        "3-5x more expensive",
        "Higher latency",
        "Still relies on LLM knowledge",
    ]
    
    def _setup(self):
        try:
            from hallucination_strategies import SelfConsistencyStrategy, mock_llm
            from scp import Verdict
            
            import os
            if os.environ.get("OPENAI_API_KEY") or os.environ.get("ANTHROPIC_API_KEY"):
                self.status = AlgorithmStatus.MOCK_ONLY
                self.status_reason = "Using mock LLM (real would cost per call)"
            else:
                self.status = AlgorithmStatus.MOCK_ONLY
                self.status_reason = "Using mock LLM (set OPENAI_API_KEY for real)"
            
            self.checker = SelfConsistencyStrategy(mock_llm, num_samples=3)
            self.Verdict = Verdict
        except ImportError as e:
            self.status = AlgorithmStatus.UNAVAILABLE
            self.status_reason = f"Missing dependency: {e}"
    
    def verify(self, claim: str) -> Tuple[Optional[bool], str, str]:
        if self.status == AlgorithmStatus.UNAVAILABLE:
            return None, "UNAVAILABLE", self.status_reason
        
        result = self.checker.check(claim)
        
        if result.verdict == self.Verdict.PASS:
            return True, "CONSISTENT", result.reasoning
        elif result.verdict == self.Verdict.FAIL:
            return False, "INCONSISTENT", result.reasoning
        else:
            return None, "UNKNOWN", result.reasoning


class KnowShowGoAlgorithm(Algorithm):
    """KnowShowGo - Cognitive Knowledge Graph."""
    
    name = "KnowShowGo"
    description = "Fuzzy ontology graph (~10ms, requires KSG server)"
    
    # Coverage capabilities (when available)
    detects_false_attribution = True
    detects_contradictions = True
    detects_extrinsic = True
    detects_intrinsic = True
    strengths = [
        "Fuzzy matching for nuanced claims",
        "Version history for auditing",
        "Weighted associations",
        "Community governance",
        "Cognitive architecture",
    ]
    weaknesses = [
        "Requires KnowShowGo server",
        "Needs initial KB population",
        "External dependency",
    ]
    
    def _setup(self):
        # KnowShowGo requires external server
        self.status = AlgorithmStatus.UNAVAILABLE
        self.status_reason = "KnowShowGo server not running (see docs/knowshowgo_integration_spec.md)"
    
    def verify(self, claim: str) -> Tuple[Optional[bool], str, str]:
        return None, "UNAVAILABLE", self.status_reason


class VerifiedMemoryAlgorithm(Algorithm):
    """Verified Memory - Cached verification with provenance."""
    
    name = "VerifiedMemory"
    description = "Cached KB + LLM fallback (~50ms avg)"
    
    # Coverage capabilities
    detects_false_attribution = True
    detects_contradictions = True
    detects_extrinsic = True
    detects_intrinsic = True
    strengths = [
        "Fast cache lookups",
        "Provenance tracking",
        "Persistent storage",
        "LLM fallback option",
    ]
    weaknesses = [
        "Cache needs warming",
        "Depends on underlying prover",
    ]
    
    def _setup(self):
        try:
            from verified_memory import VerifiedMemory, HallucinationProver, VerificationStatus
            
            self.memory = VerifiedMemory("./benchmark_memory")
            self.prover = HallucinationProver()
            self.prover.kb.add_facts_bulk([
                # Inventions
                ("Alexander Graham Bell", "invented", "the telephone"),
                ("Thomas Edison", "invented", "the light bulb"),
                ("Guglielmo Marconi", "invented", "the radio"),
                # Discoveries
                ("Albert Einstein", "discovered", "the theory of relativity"),
                ("Marie Curie", "discovered", "radium"),
                ("Isaac Newton", "discovered", "gravity"),
                ("Charles Darwin", "proposed", "the theory of evolution"),
                # Geography
                ("The Eiffel Tower", "located_in", "Paris"),
                ("The Great Wall", "located_in", "China"),
                ("The Statue of Liberty", "located_in", "New York"),
                ("The Colosseum", "located_in", "Rome"),
                ("Tokyo", "capital_of", "Japan"),
                ("Paris", "capital_of", "France"),
                ("Berlin", "capital_of", "Germany"),
                # Birth places
                ("Albert Einstein", "born_in", "Germany"),
                ("Isaac Newton", "born_in", "England"),
                # Founders
                ("Bill Gates", "founded", "Microsoft"),
                ("Steve Jobs", "founded", "Apple"),
                ("Mark Zuckerberg", "founded", "Facebook"),
                ("Guido van Rossum", "created", "Python"),
                # History
                ("George Washington", "was", "the first US president"),
                ("Shakespeare", "wrote", "Hamlet"),
                ("Beethoven", "composed", "the 9th Symphony"),
            ], source="benchmark")
            
            self.VerificationStatus = VerificationStatus
            self.status = AlgorithmStatus.AVAILABLE
            self.status_reason = "Ready with local cache"
        except ImportError as e:
            self.status = AlgorithmStatus.UNAVAILABLE
            self.status_reason = f"Missing dependency: {e}"
        except Exception as e:
            self.status = AlgorithmStatus.UNAVAILABLE
            self.status_reason = f"Setup error: {e}"
    
    def verify(self, claim: str) -> Tuple[Optional[bool], str, str]:
        if self.status == AlgorithmStatus.UNAVAILABLE:
            return None, "UNAVAILABLE", self.status_reason
        
        # Use prover.prove() method
        status, provenance, extracted_claim = self.prover.prove(claim)
        
        if status == self.VerificationStatus.VERIFIED:
            return True, "VERIFIED", provenance.source
        elif status == self.VerificationStatus.REFUTED:
            return False, "REFUTED", provenance.source
        else:
            return None, "UNVERIFIABLE", provenance.source


# =============================================================================
# BENCHMARK RUNNER
# =============================================================================

ALGORITHMS = {
    "scp": SCPAlgorithm,
    "wikidata": WikidataAlgorithm,
    "llm": LLMJudgeAlgorithm,
    "consistency": SelfConsistencyAlgorithm,
    "knowshowgo": KnowShowGoAlgorithm,
    "memory": VerifiedMemoryAlgorithm,
}


def run_benchmark(
    algorithm: Algorithm,
    test_set_name: str,
    test_set: List[Tuple[str, bool, HallucinationType]],
    verbose: bool = False
) -> TestSetResult:
    """Run benchmark on a single test set."""
    results = []
    total_start = time.perf_counter()
    
    # Track detection by type
    type_stats = {}
    for h_type in HallucinationType:
        type_stats[h_type] = {"tested": 0, "detected": 0}
    
    for claim, expected, h_type in test_set:
        start = time.perf_counter()
        actual, verdict, reason = algorithm.verify(claim)
        latency = (time.perf_counter() - start) * 1000
        
        # Determine if passed
        if actual is None:
            passed = False  # Unverifiable counts as fail for benchmark
        else:
            passed = actual == expected
        
        # Track detection by hallucination type
        type_stats[h_type]["tested"] += 1
        if passed:
            type_stats[h_type]["detected"] += 1
        
        results.append(ClaimResult(
            claim=claim,
            expected=expected,
            actual=actual,
            passed=passed,
            latency_ms=latency,
            verdict=verdict,
            reason=reason[:100] if reason else "",
            hallucination_type=h_type
        ))
        
        if verbose:
            status = "‚úì" if passed else "‚úó"
            actual_str = "TRUE" if actual else ("FALSE" if actual is False else "N/A")
            expected_str = "TRUE" if expected else "FALSE"
            type_label = h_type.value
            print(f"  {status} [{type_label:<12}] {claim[:40]}...")
            print(f"      Expected: {expected_str}, Got: {actual_str} ({verdict})")
            
            # Show inline guidance for failures
            if not passed and h_type in IMPLEMENTATION_GUIDANCE:
                guidance = IMPLEMENTATION_GUIDANCE[h_type]
                print(f"      ‚ö†Ô∏è  {guidance['description']}")
                print(f"      üí° Example: {guidance['example']}")
    
    total_time = (time.perf_counter() - total_start) * 1000
    passed_count = sum(1 for r in results if r.passed)
    
    # Calculate detection rates by type
    detection_by_type = {}
    for h_type, stats in type_stats.items():
        if stats["tested"] > 0:
            detection_by_type[h_type] = TypeDetectionRate(
                hallucination_type=h_type,
                tested=stats["tested"],
                detected=stats["detected"],
                rate=stats["detected"] / stats["tested"]
            )
    
    return TestSetResult(
        name=test_set_name,
        claims_tested=len(results),
        passed=passed_count,
        failed=len(results) - passed_count,
        accuracy=passed_count / len(results) if results else 0,
        avg_latency_ms=sum(r.latency_ms for r in results) / len(results) if results else 0,
        total_time_ms=total_time,
        results=results,
        detection_by_type=detection_by_type
    )


def run_algorithm_benchmark(
    alg_class: type,
    test_sets: Dict[str, Tuple[str, List]],
    verbose: bool = False
) -> AlgorithmResult:
    """Run full benchmark for an algorithm."""
    algorithm = alg_class()
    
    print(f"\n{'=' * 70}")
    print(f"ALGORITHM: {algorithm.name}")
    print(f"{'=' * 70}")
    print(f"Description: {algorithm.description}")
    print(f"Status: {algorithm.status.value} - {algorithm.status_reason}")
    
    if algorithm.status == AlgorithmStatus.UNAVAILABLE:
        coverage = algorithm.get_coverage_metrics({}, 0.0, {})
        return AlgorithmResult(
            name=algorithm.name,
            description=algorithm.description,
            status=algorithm.status,
            status_reason=algorithm.status_reason,
            test_sets={},
            overall_accuracy=0.0,
            overall_latency_ms=0.0,
            coverage=coverage,
            timestamp=datetime.now().isoformat()
        )
    
    set_results = {}
    all_type_stats = {}  # Aggregate detection by type across all test sets
    
    for set_key, (set_name, test_data) in test_sets.items():
        print(f"\n  Test Set: {set_name}")
        print(f"  {'-' * 50}")
        result = run_benchmark(algorithm, set_name, test_data, verbose)
        set_results[set_key] = result
        print(f"  Results: {result.passed}/{result.claims_tested} ({result.accuracy:.0%})")
        print(f"  Avg Latency: {result.avg_latency_ms:.1f}ms")
        
        # Aggregate type stats
        for h_type, rate in result.detection_by_type.items():
            if h_type not in all_type_stats:
                all_type_stats[h_type] = {"tested": 0, "detected": 0}
            all_type_stats[h_type]["tested"] += rate.tested
            all_type_stats[h_type]["detected"] += rate.detected
    
    # Calculate overall metrics
    total_passed = sum(r.passed for r in set_results.values())
    total_claims = sum(r.claims_tested for r in set_results.values())
    total_latency = sum(r.avg_latency_ms * r.claims_tested for r in set_results.values())
    overall_accuracy = total_passed / total_claims if total_claims else 0
    
    # Calculate actual detection rates by type
    actual_detection_rates = {}
    for h_type, stats in all_type_stats.items():
        if stats["tested"] > 0:
            actual_detection_rates[h_type] = stats["detected"] / stats["tested"]
    
    # Calculate domain coverage scores
    domain_scores = {k: r.accuracy for k, r in set_results.items()}
    
    # Get coverage metrics with actual detection rates
    coverage = algorithm.get_coverage_metrics(domain_scores, overall_accuracy, actual_detection_rates)
    
    # Print actual detection coverage
    print(f"\n  Detection Coverage (from tests):")
    print(f"  {'-' * 50}")
    for h_type in [HallucinationType.TRUE_FACT, HallucinationType.FALSE_ATTRIBUTION, 
                   HallucinationType.CONTRADICTION, HallucinationType.FABRICATION]:
        if h_type in all_type_stats and all_type_stats[h_type]["tested"] > 0:
            stats = all_type_stats[h_type]
            rate = stats["detected"] / stats["tested"]
            bar = "‚ñà" * int(rate * 10) + "‚ñë" * (10 - int(rate * 10))
            print(f"  {h_type.value:<15} {bar} {rate:>5.0%} ({stats['detected']}/{stats['tested']})")
    
    # Print coverage summary
    print(f"\n  Overall Coverage: {coverage.level.value.upper()} (score: {coverage.score:.0f}/100)")
    
    return AlgorithmResult(
        name=algorithm.name,
        description=algorithm.description,
        status=algorithm.status,
        status_reason=algorithm.status_reason,
        test_sets=set_results,
        overall_accuracy=overall_accuracy,
        overall_latency_ms=total_latency / total_claims if total_claims else 0,
        coverage=coverage,
        timestamp=datetime.now().isoformat()
    )


def print_implementation_guidance(results: List[AlgorithmResult], verbose: bool = False):
    """Print what needs to be built to improve detection coverage."""
    print("\n" + "=" * 70)
    print("IMPLEMENTATION GUIDANCE - What to Build")
    print("=" * 70)
    
    # Collect all failures by type across algorithms
    failures_by_type = {h_type: [] for h_type in HallucinationType}
    failures_by_algo = {}
    
    for r in results:
        if r.status == AlgorithmStatus.UNAVAILABLE:
            failures_by_algo[r.name] = {
                "unavailable": True,
                "reason": r.status_reason
            }
            continue
        
        failures_by_algo[r.name] = {
            "unavailable": False,
            "failed_types": []
        }
        
        for set_result in r.test_sets.values():
            for claim_result in set_result.results:
                if not claim_result.passed:
                    failures_by_type[claim_result.hallucination_type].append({
                        "algo": r.name,
                        "claim": claim_result.claim,
                        "expected": claim_result.expected,
                        "got": claim_result.actual,
                        "verdict": claim_result.verdict,
                    })
                    failures_by_algo[r.name]["failed_types"].append(claim_result.hallucination_type)
    
    # Print urgent fixes (unavailable algorithms)
    unavailable = [a for a, info in failures_by_algo.items() if info.get("unavailable")]
    if unavailable:
        print("\n‚ö†Ô∏è  UNAVAILABLE ALGORITHMS - Require Setup")
        print("-" * 60)
        for algo in unavailable:
            print(f"\n  {algo}: {failures_by_algo[algo]['reason']}")
            if algo in ALGORITHM_IMPLEMENTATION_NEEDS:
                for fix_key, fix_text in ALGORITHM_IMPLEMENTATION_NEEDS[algo].items():
                    if "server" in fix_key or "api" in fix_key:
                        print(fix_text)
    
    # Print by hallucination type
    print("\nüìã FAILURES BY HALLUCINATION TYPE")
    print("-" * 60)
    
    for h_type in [HallucinationType.FALSE_ATTRIBUTION, HallucinationType.CONTRADICTION, 
                   HallucinationType.FABRICATION, HallucinationType.TRUE_FACT]:
        type_failures = failures_by_type[h_type]
        if not type_failures:
            print(f"\n  ‚úì {h_type.value}: All tests passing!")
            continue
        
        print(f"\n  ‚úó {h_type.value}: {len(type_failures)} failures")
        
        # Show specific failures if verbose
        if verbose:
            for f in type_failures[:3]:  # Show first 3
                expected_str = "TRUE" if f["expected"] else "FALSE"
                got_str = "TRUE" if f["got"] else ("FALSE" if f["got"] is False else "N/A")
                print(f"      ‚Ä¢ [{f['algo']}] \"{f['claim'][:40]}...\"")
                print(f"        Expected: {expected_str}, Got: {got_str} ({f['verdict']})")
        
        # Show implementation guidance
        if h_type in IMPLEMENTATION_GUIDANCE:
            guidance = IMPLEMENTATION_GUIDANCE[h_type]
            print(f"\n    üìù To fix {h_type.value}:")
            print(f"    {guidance['description']}")
            if verbose:
                print(guidance['fix'])
    
    # Print algorithm-specific fixes
    print("\nüîß ALGORITHM-SPECIFIC FIXES")
    print("-" * 60)
    
    for algo, info in failures_by_algo.items():
        if info.get("unavailable"):
            continue
        
        failed_types = list(set(info.get("failed_types", [])))
        if not failed_types:
            print(f"\n  ‚úì {algo}: All tests passing!")
            continue
        
        print(f"\n  {algo}: {len(failed_types)} type(s) with failures")
        
        # Show algorithm-specific implementation needs
        if algo in ALGORITHM_IMPLEMENTATION_NEEDS:
            for fix_key, fix_text in ALGORITHM_IMPLEMENTATION_NEEDS[algo].items():
                if "false_attr" in fix_key and HallucinationType.FALSE_ATTRIBUTION in failed_types:
                    print(f"\n    Fix for false attribution:")
                    if verbose:
                        print(fix_text)
                    else:
                        print(f"    (run with --verbose to see implementation details)")
                elif "predicate" in fix_key and HallucinationType.CONTRADICTION in failed_types:
                    print(f"\n    Fix for contradiction detection:")
                    if verbose:
                        print(fix_text)
                    else:
                        print(f"    (run with --verbose to see implementation details)")
    
    # Print quick summary
    print("\n" + "=" * 70)
    print("üéØ PRIORITY ACTION ITEMS")
    print("=" * 70)
    
    priority_items = []
    
    # Check for false attribution failures
    fa_failures = len(failures_by_type[HallucinationType.FALSE_ATTRIBUTION])
    if fa_failures > 0:
        priority_items.append(("HIGH", f"Fix false attribution ({fa_failures} failures)", "1-2 hours", 
            "Update scp.py: Add subject-aware matching in find_similar_facts()"))
    
    # Check for contradiction failures
    cont_failures = len(failures_by_type[HallucinationType.CONTRADICTION])
    if cont_failures > 0:
        priority_items.append(("HIGH", f"Fix contradiction detection ({cont_failures} failures)", "1-2 hours",
            "Expand KB facts: birth places, locations, capitals"))
    
    # Check for unavailable algorithms
    if "KnowShowGo" in unavailable:
        priority_items.append(("MEDIUM", "Deploy KnowShowGo server", "4-8 hours",
            "See docs/knowshowgo_integration_spec.md"))
    
    if "LLM-Judge" in [a for a in failures_by_algo if failures_by_algo[a].get("unavailable")]:
        priority_items.append(("LOW", "Enable real LLM", "30 mins",
            "Set OPENAI_API_KEY environment variable"))
    
    if priority_items:
        print(f"\n{'Priority':<8} {'Task':<45} {'Time':<10} Action")
        print("-" * 80)
        for priority, task, time_est, action in priority_items:
            print(f"{priority:<8} {task:<45} {time_est:<10} {action}")
    else:
        print("\n  üéâ All systems operational - no urgent fixes needed!")


def print_summary(results: List[AlgorithmResult]):
    """Print benchmark summary."""
    print("\n" + "=" * 70)
    print("BENCHMARK SUMMARY")
    print("=" * 70)
    
    print(f"\n{'Algorithm':<16} {'Status':<11} {'Accuracy':<9} {'Latency':<10} {'Coverage':<12} {'Score'}")
    print("-" * 75)
    
    for r in results:
        status = r.status.value[:10]
        coverage_level = r.coverage.level.value if r.coverage else "N/A"
        coverage_score = f"{r.coverage.score:.0f}/100" if r.coverage else "N/A"
        
        if r.status == AlgorithmStatus.AVAILABLE:
            print(f"{r.name:<16} {status:<11} {r.overall_accuracy:>5.0%}     {r.overall_latency_ms:>6.1f}ms   {coverage_level:<12} {coverage_score}")
        elif r.status == AlgorithmStatus.MOCK_ONLY:
            print(f"{r.name:<16} {status:<11} {r.overall_accuracy:>5.0%}*    {r.overall_latency_ms:>6.1f}ms   {coverage_level:<12} {coverage_score}")
        else:
            print(f"{r.name:<16} {status:<11} {'N/A':>5}     {'N/A':>6}    {coverage_level:<12} {coverage_score}")
    
    print("\n* = Using mock implementation")
    
    # Print coverage legend
    print("\nCoverage Levels:")
    print("  EXCELLENT (90%+): Comprehensive detection across all hallucination types")
    print("  GOOD (70-89%):    Reliable detection for most claim types")
    print("  MODERATE (50-69%): Partial detection, some blind spots")
    print("  WEAK (30-49%):    Limited reliability, use with caution")
    print("  MINIMAL (<30%):   Not recommended for production use")


def export_markdown(results: List[AlgorithmResult]) -> str:
    """Export results as markdown table."""
    lines = [
        "## Benchmark Results",
        "",
        f"*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*",
        "",
        "### Algorithm Comparison",
        "",
        "| Algorithm | Status | Accuracy | Coverage | Score | Latency |",
        "|-----------|--------|----------|----------|-------|---------|",
    ]
    
    for r in results:
        status = r.status.value
        coverage = r.coverage.level.value.upper() if r.coverage else "N/A"
        score = f"{r.coverage.score:.0f}/100" if r.coverage else "N/A"
        if r.status == AlgorithmStatus.UNAVAILABLE:
            acc = "N/A"
            lat = "N/A"
        else:
            acc = f"{r.overall_accuracy:.0%}"
            lat = f"{r.overall_latency_ms:.1f}ms"
        lines.append(f"| {r.name} | {status} | {acc} | {coverage} | {score} | {lat} |")
    
    lines.extend([
        "",
        "### Coverage Legend",
        "",
        "| Level | Accuracy | Description |",
        "|-------|----------|-------------|",
        "| EXCELLENT | 90%+ | Comprehensive detection across all types |",
        "| GOOD | 70-89% | Reliable for most claim types |",
        "| MODERATE | 50-69% | Partial detection, some blind spots |",
        "| WEAK | 30-49% | Limited reliability |",
        "| MINIMAL | <30% | Not recommended for production |",
        "",
        "### Detection Capabilities",
        "",
        "| Algorithm | False Attribution | Contradictions | Extrinsic | Intrinsic |",
        "|-----------|-------------------|----------------|-----------|-----------|",
    ])
    
    for r in results:
        if r.coverage:
            fa = "‚úì" if r.coverage.detects_false_attribution else "‚úó"
            co = "‚úì" if r.coverage.detects_contradictions else "‚úó"
            ex = "‚úì" if r.coverage.detects_extrinsic else "‚úó"
            int_ = "‚úì" if r.coverage.detects_intrinsic else "‚úó"
            lines.append(f"| {r.name} | {fa} | {co} | {ex} | {int_} |")
    
    lines.extend([
        "",
        "### Strengths & Weaknesses",
        "",
    ])
    
    for r in results:
        if r.coverage and (r.coverage.strengths or r.coverage.weaknesses):
            lines.append(f"#### {r.name}")
            lines.append("")
            if r.coverage.strengths:
                lines.append("**Strengths:**")
                for s in r.coverage.strengths:
                    lines.append(f"- {s}")
            if r.coverage.weaknesses:
                lines.append("")
                lines.append("**Weaknesses:**")
                for w in r.coverage.weaknesses:
                    lines.append(f"- {w}")
            lines.append("")
    
    lines.extend([
        "### Test Set Breakdown",
        "",
    ])
    
    for r in results:
        if r.status == AlgorithmStatus.UNAVAILABLE:
            continue
        lines.append(f"#### {r.name}")
        lines.append("")
        lines.append("| Test Set | Passed | Total | Accuracy | Avg Latency |")
        lines.append("|----------|--------|-------|----------|-------------|")
        for set_key, set_result in r.test_sets.items():
            lines.append(
                f"| {set_result.name} | {set_result.passed} | {set_result.claims_tested} | "
                f"{set_result.accuracy:.0%} | {set_result.avg_latency_ms:.1f}ms |"
            )
        lines.append("")
    
    # Add Implementation Guidance section
    lines.extend([
        "### Implementation Guidance",
        "",
        "When tests fail, here's what to implement:",
        "",
    ])
    
    for h_type in [HallucinationType.FALSE_ATTRIBUTION, HallucinationType.CONTRADICTION,
                   HallucinationType.FABRICATION]:
        if h_type in IMPLEMENTATION_GUIDANCE:
            guidance = IMPLEMENTATION_GUIDANCE[h_type]
            lines.extend([
                f"#### {h_type.value}",
                "",
                f"**Problem:** {guidance['description']}",
                f"**Example:** {guidance['example']}",
                "",
                "**Solution:**",
                "```python",
                guidance['fix'].strip(),
                "```",
                "",
            ])
    
    # Add algorithm-specific needs
    lines.extend([
        "### Algorithm-Specific Requirements",
        "",
    ])
    
    for algo_name, needs in ALGORITHM_IMPLEMENTATION_NEEDS.items():
        lines.append(f"#### {algo_name}")
        lines.append("")
        for fix_key, fix_text in needs.items():
            lines.append("```")
            lines.append(fix_text.strip())
            lines.append("```")
            lines.append("")
    
    return "\n".join(lines)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Benchmark hallucination detection algorithms"
    )
    parser.add_argument(
        "--algorithm", "-a",
        choices=list(ALGORITHMS.keys()) + ["all"],
        default="all",
        help="Which algorithm to benchmark"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Show detailed output for each claim"
    )
    parser.add_argument(
        "--export", "-e",
        action="store_true",
        help="Export results as markdown"
    )
    args = parser.parse_args()
    
    print("=" * 70)
    print("HALLUCINATION DETECTION BENCHMARK SUITE")
    print("Author: Lehel Kovach | AI: Claude Opus 4.5")
    print("=" * 70)
    print(f"\nRunning benchmarks with {len(TEST_SETS)} test sets...")
    
    results = []
    
    if args.algorithm == "all":
        for name, alg_class in ALGORITHMS.items():
            result = run_algorithm_benchmark(alg_class, TEST_SETS, args.verbose)
            results.append(result)
    else:
        alg_class = ALGORITHMS[args.algorithm]
        result = run_algorithm_benchmark(alg_class, TEST_SETS, args.verbose)
        results.append(result)
    
    print_summary(results)
    
    # Always print implementation guidance
    print_implementation_guidance(results, args.verbose)
    
    if args.export:
        markdown = export_markdown(results)
        print("\n" + "=" * 70)
        print("MARKDOWN EXPORT")
        print("=" * 70)
        print(markdown)
        
        # Save to file
        with open("BENCHMARK_RESULTS.md", "w") as f:
            f.write(markdown)
        print(f"\nSaved to BENCHMARK_RESULTS.md")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
